{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for random operations. \n",
    "# This let our experiments to be reproducible. \n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Set GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation - Next character prediction\n",
    "## Charles Dickens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "# ---------------\n",
    "\n",
    "# Read full text\n",
    "with open(os.path.join(cwd, 'dickens.txt'), 'r') as f:\n",
    "    full_text = f.read()\n",
    "f.close()\n",
    "\n",
    "full_text_length = len(full_text)\n",
    "print('Full text length:', full_text_length)\n",
    "\n",
    "# Create vocabulary\n",
    "vocabulary = sorted(list(set(full_text)))\n",
    "\n",
    "print('Number of unique characters:', len(vocabulary))\n",
    "print(vocabulary)\n",
    "\n",
    "# Dictionaries for char-to-int/int-to-char conversion\n",
    "ctoi = {c:i for i, c in enumerate(vocabulary)}\n",
    "itoc = {i:c for i, c in enumerate(vocabulary)}\n",
    "\n",
    "# Create input-target pairs\n",
    "# e.g., given an input sequence \n",
    "# 'Hell' predict the next character 'o'\n",
    "# Thus,\n",
    "# extract from the full text sequences of length seq_length as x and \n",
    "# the corresponding seq_length+1 character as target\n",
    "\n",
    "# Define number of characters to be considered for the prediction\n",
    "seq_length = 100\n",
    "\n",
    "X = [] # will contain all the sequences \n",
    "Y = [] # will contain for each sequence in X the corresponding expected next character\n",
    "# Cycle over the full text\n",
    "step = 1 \n",
    "for i in range(0, full_text_length - (seq_length), step):\n",
    "    sequence = full_text[i:i+seq_length]\n",
    "    target = full_text[i+seq_length]\n",
    "    X.append([ctoi[c] for c in sequence])\n",
    "    Y.append(ctoi[target])\n",
    "    \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "    \n",
    "print('Number of sequences in the dataset:', len(X))\n",
    "\n",
    "# Shuffle Dataset\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "X = X[indices]\n",
    "Y = Y[indices]\n",
    "\n",
    "# Divide into training and validation sets\n",
    "# e.g., ~90% for training and ~10% for validation\n",
    "num_train = int(0.9*len(X))\n",
    "x_train = X[:num_train] \n",
    "y_train = Y[:num_train]\n",
    "x_valid = X[num_train:]\n",
    "y_valid = Y[num_train:]\n",
    "\n",
    "# convert in numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_valid = np.array(x_valid)\n",
    "y_valid = np.array(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "# -------------------\n",
    "\n",
    "# Batch size\n",
    "bs = 256\n",
    "\n",
    "# Encode characters. Many ways, for example one-hot encoding.\n",
    "def char_encode(x_, y_):\n",
    "    return tf.one_hot(x_, len(vocabulary)), tf.one_hot(y_, len(vocabulary))\n",
    "\n",
    "# Prepare input x to match recurrent layer input shape \n",
    "# -> (bs, seq_length, input_size)\n",
    "\n",
    "# Training\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=x_train.shape[0])\n",
    "train_dataset = train_dataset.map(char_encode)\n",
    "train_dataset = train_dataset.batch(bs)\n",
    "train_dataset = train_dataset.repeat()\n",
    "\n",
    "# Validation\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n",
    "valid_dataset = valid_dataset.map(char_encode)\n",
    "valid_dataset = valid_dataset.batch(bs)\n",
    "valid_dataset = valid_dataset.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Recurrent Neural Network\n",
    "# ------------------------------\n",
    "\n",
    "# Hidden size (state)\n",
    "h_size = 128\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(units=h_size, batch_input_shape=[None, seq_length, len(vocabulary)], \n",
    "                               return_sequences=True, stateful=False))\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "model.add(tf.keras.layers.LSTM(units=h_size, return_sequences=False, stateful=False))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "# model.add(tf.keras.layers.LSTM(units=h_size, return_sequences=False)\n",
    "model.add(tf.keras.layers.Dense(units=len(vocabulary), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization params\n",
    "# -------------------\n",
    "\n",
    "# Loss\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-2\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# -------------------\n",
    "\n",
    "# Validation metrics\n",
    "# ------------------\n",
    "\n",
    "metrics = ['accuracy']\n",
    "# ------------------\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "exps_dir = os.path.join(cwd, 'dickens_experiments')\n",
    "if not os.path.exists(exps_dir):\n",
    "    os.makedirs(exps_dir)\n",
    "\n",
    "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "exp_name = 'exp'\n",
    "\n",
    "exp_dir = os.path.join(exps_dir, exp_name + '_' + str(now))\n",
    "if not os.path.exists(exp_dir):\n",
    "    os.makedirs(exp_dir)\n",
    "    \n",
    "callbacks = []\n",
    "\n",
    "# Model checkpoint\n",
    "# ----------------\n",
    "ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "\n",
    "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n",
    "                                                   save_weights_only=True)  # False to save the model directly\n",
    "callbacks.append(ckpt_callback)\n",
    "\n",
    "# ----------------\n",
    "\n",
    "# Visualize Learning on Tensorboard\n",
    "# ---------------------------------\n",
    "tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
    "if not os.path.exists(tb_dir):\n",
    "    os.makedirs(tb_dir)\n",
    "    \n",
    "# By default shows losses and metrics for both training and validation\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
    "                                             profile_batch=0,\n",
    "                                             histogram_freq=1)  # if 1 shows weights histograms\n",
    "callbacks.append(tb_callback)\n",
    "\n",
    "# Early Stopping\n",
    "# --------------\n",
    "early_stop = False\n",
    "if early_stop:\n",
    "    es_callback = tf.keras.callback.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    callbacks.append(es_callback)\n",
    "\n",
    "# ---------------------------------\n",
    "\n",
    "model.fit(x=train_dataset,\n",
    "          epochs=100,  #### set repeat in training dataset\n",
    "          steps_per_epoch=int(np.ceil(x_train.shape[0] / bs)),\n",
    "          validation_data=valid_dataset,\n",
    "          validation_steps=int(np.ceil(x_valid.shape[0] / bs)), \n",
    "          callbacks=callbacks)\n",
    "\n",
    "# How to visualize Tensorboard\n",
    "\n",
    "# 1. tensorboard --logdir EXPERIMENTS_DIR --port PORT     <- from terminal\n",
    "# 2. localhost:PORT   <- in your browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
