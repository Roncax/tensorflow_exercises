{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for random operations. \n",
    "# This let our experiments to be reproducible. \n",
    "SEED = 12\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Set GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation - Next character prediction\n",
    "## Charles Dickens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "# ---------------\n",
    "\n",
    "# Read full text\n",
    "with open(os.path.join(cwd, 'dickens.txt'), 'r') as f:\n",
    "    full_text = f.read()\n",
    "f.close()\n",
    "\n",
    "full_text_length = len(full_text)\n",
    "print('Full text length:', full_text_length)\n",
    "\n",
    "# Create vocabulary\n",
    "vocabulary = sorted(list(set(full_text)))\n",
    "\n",
    "print('Number of unique characters:', len(vocabulary))\n",
    "print(vocabulary)\n",
    "\n",
    "# Dictionaries for char-to-int/int-to-char conversion\n",
    "ctoi = {c:i for i, c in enumerate(vocabulary)}\n",
    "itoc = {i:c for i, c in enumerate(vocabulary)}\n",
    "\n",
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Recurrent Neural Network\n",
    "# ------------------------------\n",
    "\n",
    "# Hidden size (state)\n",
    "h_size = 128\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(units=h_size, batch_input_shape=[None, seq_length, len(vocabulary)], \n",
    "                               return_sequences=True, stateful=False))\n",
    "model.add(tf.keras.layers.LSTM(units=h_size, return_sequences=False, stateful=False))\n",
    "model.add(tf.keras.layers.Dense(units=len(vocabulary), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization params\n",
    "# -------------------\n",
    "\n",
    "# Loss\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-2\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# -------------------\n",
    "\n",
    "# Validation metrics\n",
    "# ------------------\n",
    "\n",
    "metrics = ['accuracy']\n",
    "# ------------------\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of characters we want to generate\n",
    "generation_length = 100\n",
    "\n",
    "# Get random seed sequence\n",
    "start_idx = np.random.randint(0, full_text_length - seq_length)\n",
    "\n",
    "seed_sentence = full_text[start_idx:start_idx+seq_length]\n",
    "\n",
    "print('----- Seed sequence:')\n",
    "print(seed_sentence)\n",
    "\n",
    "in_onehot = np.zeros([1, seq_length, len(vocabulary)])\n",
    "for t_idx, c in enumerate(seed_sentence):\n",
    "    in_onehot[:, t_idx, ctoi[c]] = 1.\n",
    "    \n",
    "generated_sentence = seed_sentence\n",
    "    \n",
    "for i in range(generation_length):\n",
    "        \n",
    "    preds = model.predict(in_onehot, verbose=0)[0]\n",
    "    \n",
    "    # Two main ways of predicting\n",
    "    # dummy: argmax\n",
    "    # next_char = np.argmax(preds[-1], temperature=0.5)\n",
    "    # sampling\n",
    "    # less the temperature more predictable is the output\n",
    "    next_char = sample(preds, temperature=0.5)  # next_char is the id\n",
    "    \n",
    "    next_char_onehot = np.zeros([1, 1, len(vocabulary)])\n",
    "    next_char_onehot[:, :, next_char] = 1.\n",
    "    \n",
    "    in_onehot = np.concatenate([in_onehot, next_char_onehot], axis=1)\n",
    "    in_onehot = in_onehot[:, 1:, :]\n",
    "    \n",
    "    generated_sentence += itoc[next_char]\n",
    "\n",
    "print('\\n----- Generated Sentence')\n",
    "print(generated_sentence)\n",
    "\n",
    "print('\\n----- Original Sentence')\n",
    "original_sentence = full_text[start_idx:start_idx+len(generated_sentence)]\n",
    "print(original_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used to visualize different results at different epochs (starting, intermediate, final)\n",
    "# ADAPT THIS TO YOUR CODE\n",
    "for epoch in range(1, 101, 49):\n",
    "    \n",
    "    print('\\nModel epoch:', epoch)\n",
    "    print('###############')\n",
    "    \n",
    "    if epoch < 10:\n",
    "        epoch_str = '0'+str(epoch)\n",
    "    else:\n",
    "        epoch_str = str(epoch)\n",
    "    \n",
    "    # Load Model at current epoch\n",
    "    model.load_weights(os.path.join(\n",
    "        cwd, 'dickens_experiments', 'exp_Dec12_22-18-56', 'ckpts', 'cp_'+epoch_str+'.ckpt'))\n",
    "    \n",
    "    print('\\n----- Seed sequence:')\n",
    "    print(seed_sentence)\n",
    "\n",
    "    in_onehot = np.zeros([1, seq_length, len(vocabulary)])\n",
    "    for t_idx, c in enumerate(seed_sentence):\n",
    "        in_onehot[:, t_idx, ctoi[c]] = 1.\n",
    "\n",
    "    generated_sentence = seed_sentence\n",
    "\n",
    "    for i in range(generation_length):\n",
    "\n",
    "        preds = model.predict(in_onehot, verbose=0)[0]\n",
    "\n",
    "        # Two main ways of predicting\n",
    "        # dummy: argmax\n",
    "        # next_char = np.argmax(preds[-1])\n",
    "        # sampling\n",
    "        # less the temperature more predictable is the output\n",
    "        next_char = sample(preds, temperature=0.5)  # next_char is the id\n",
    "\n",
    "        next_char_onehot = np.zeros([1, 1, len(vocabulary)])\n",
    "        next_char_onehot[:, :, next_char] = 1.\n",
    "\n",
    "        in_onehot = np.concatenate([in_onehot, next_char_onehot], axis=1)\n",
    "        in_onehot = in_onehot[:, 1:, :]\n",
    "\n",
    "        generated_sentence += itoc[next_char]\n",
    "\n",
    "\n",
    "    print('\\n----- Generated Sentence')\n",
    "    print(generated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize most probable future characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "# Load Model at wanted epoch\n",
    "model.load_weights(os.path.join(\n",
    "    cwd, 'dickens_experiments', 'exp_Dec12_22-18-56', 'ckpts', 'cp_100.ckpt'))\n",
    "\n",
    "# Get random slice from text of length 2*seq_length\n",
    "start_idx = np.random.randint(0, full_text_length - seq_length)\n",
    "\n",
    "text_slice = full_text[start_idx:start_idx+2*seq_length]\n",
    "\n",
    "seed_sentence = text_slice[:seq_length]\n",
    "\n",
    "in_onehot = np.zeros([1, seq_length, len(vocabulary)])\n",
    "for t_idx, c in enumerate(seed_sentence):\n",
    "    in_onehot[:, t_idx, ctoi[c]] = 1.\n",
    "\n",
    "print('\\n----- Seed sequence:')\n",
    "print(seed_sentence)\n",
    "\n",
    "next_char = text_slice[seq_length-1]\n",
    "\n",
    "next_chars = []\n",
    "next_chars.append(next_char)\n",
    "probs = []\n",
    "\n",
    "for i in range(seq_length):\n",
    "\n",
    "    preds = model.predict(in_onehot, verbose=0)[0]\n",
    "    \n",
    "    ordered_preds = np.argsort(preds)[::-1]\n",
    "   \n",
    "    probs.append([itoc[ordered_preds[0]], itoc[ordered_preds[1]], \n",
    "          itoc[ordered_preds[2]], itoc[ordered_preds[3]], \n",
    "          itoc[ordered_preds[4]]])\n",
    "          \n",
    "    next_char = text_slice[seq_length+i]\n",
    "    next_chars.append(next_char)\n",
    "    next_char_id = ctoi[next_char]\n",
    "\n",
    "    next_char_onehot = np.zeros([1, 1, len(vocabulary)])\n",
    "    next_char_onehot[:, :, next_char_id] = 1.\n",
    "\n",
    "    in_onehot = np.concatenate([in_onehot, next_char_onehot], axis=1)\n",
    "    in_onehot = in_onehot[:, 1:, :]\n",
    "    \n",
    "plt.figure(figsize=(20,2))\n",
    "clust_data = np.array(probs).T\n",
    "collabel=next_chars\n",
    "table = plt.table(cellText=clust_data,colLabels=collabel, loc='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize LSTM hidden neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "# Load Model at wanted epoch\n",
    "model.load_weights(os.path.join(\n",
    "   cwd, 'dickens_experiments', 'exp_Dec12_22-18-56', 'ckpts', 'cp_100.ckpt'))\n",
    "\n",
    "# Create a new model to get neurons activations\n",
    "model_in = model.input \n",
    "model_out = model.layers[1].output # Which recurrent layer (in this case last one)\n",
    "new_model = tf.keras.Model(model_in, model_out)\n",
    "\n",
    "# Get random slice from text of length 2*seq_length\n",
    "start_idx = np.random.randint(0, full_text_length - seq_length)\n",
    "\n",
    "text_slice = full_text[start_idx:start_idx+2*seq_length]\n",
    "\n",
    "seed_sentence = text_slice[:seq_length]\n",
    "\n",
    "in_onehot = np.zeros([1, seq_length, len(vocabulary)])\n",
    "for t_idx, c in enumerate(seed_sentence):\n",
    "    in_onehot[:, t_idx, ctoi[c]] = 1.\n",
    "\n",
    "print('\\n----- Seed sequence:')\n",
    "print(seed_sentence)\n",
    "\n",
    "next_char = text_slice[seq_length-1]\n",
    "\n",
    "next_chars = []\n",
    "next_chars.append(next_char)\n",
    "neuron_values = []\n",
    "\n",
    "which_neuron = 8 # which neuron we want to inspect\n",
    "\n",
    "for i in range(seq_length):\n",
    "\n",
    "    lstm_states = new_model.predict(in_onehot, verbose=0)[0]\n",
    "    lstm_final_state = lstm_states\n",
    "    lstm_neuron = lstm_final_state[which_neuron]\n",
    "    neuron_values.append(lstm_neuron)\n",
    "\n",
    "    next_char = text_slice[seq_length+i]\n",
    "    next_chars.append(next_char)\n",
    "    next_char_id = ctoi[next_char]\n",
    "\n",
    "    next_char_onehot = np.zeros([1, 1, len(vocabulary)])\n",
    "    next_char_onehot[:, :, next_char_id] = 1.\n",
    "\n",
    "    in_onehot = np.concatenate([in_onehot, next_char_onehot], axis=1)\n",
    "    in_onehot = in_onehot[:, 1:, :]\n",
    "    \n",
    "plt.figure(figsize=(20,2))\n",
    "clust_data = np.expand_dims(np.array(neuron_values), -1).T\n",
    "collabel=next_chars\n",
    "norm = plt.Normalize(min(neuron_values)-1, max(neuron_values)+1)\n",
    "colours = plt.cm.hot(norm(neuron_values))\n",
    "viridis = cm.get_cmap('PuBu_r', 100)\n",
    "table = plt.table(cellText=np.expand_dims(np.array(collabel[:-1]), -1).T,\n",
    "                  cellColours=np.expand_dims(colours, 0), # np.expand_dims(neuron_values, -1).T,\n",
    "                  # colLabels=collabel, \n",
    "                  # colColours=['b','g','r'], \n",
    "                  loc='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
