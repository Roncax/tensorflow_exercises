{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"FashionMNIST.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"ADZK2DqC9kpf","colab_type":"code","colab":{}},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EtLg9kcD9kpn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"0439af67-482d-4d14-8c41-17bb190f3a83","executionInfo":{"status":"ok","timestamp":1574423773148,"user_tz":-60,"elapsed":8409,"user":{"displayName":"Paolo Roncaglioni","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC3-ttkSuo6iETyKDuUhOcjyUtrv79Oa99mCxYr9h4=s64","userId":"17876907246980835885"}}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","# Install TensorFlow\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf\n","import numpy as np\n","\n","# Set the seed for random operations. \n","# This let our experiments to be reproducible. \n","tf.random.set_seed(1234)  "],"execution_count":2,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"52p8Wi5f9kpw","colab_type":"text"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"CyCwsbXM9kpx","colab_type":"text"},"source":["### tf.data.Dataset.range"]},{"cell_type":"code","metadata":{"id":"AWTyM99M9kp0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"ea0a1b70-72da-4219-958a-6e51ef669655","executionInfo":{"status":"ok","timestamp":1574423773854,"user_tz":-60,"elapsed":9107,"user":{"displayName":"Paolo Roncaglioni","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC3-ttkSuo6iETyKDuUhOcjyUtrv79Oa99mCxYr9h4=s64","userId":"17876907246980835885"}}},"source":["# Create a Dataset of sequential numbers\n","# --------------------------------------\n","print(\"Dataset.range examples:\")\n","print(\"-----------------------\")\n","\n","range_dataset = tf.data.Dataset.range(0, 20, 1)\n","\n","print(\"\\n1. Dataset\")\n","for el in range_dataset:\n","    print(el)\n","\n","# Divide in batches\n","bs = 3\n","range_dataset = tf.data.Dataset.range(0, 20, 1).batch(bs, drop_remainder=False)\n","\n","print(\"\\n2. Dataset + batch\")\n","for el in range_dataset:\n","    print(el)\n","\n","# Apply a transformation to each element\n","def map_fn(x):\n","    return x**2\n","\n","range_dataset = tf.data.Dataset.range(0, 20, 1).batch(bs, drop_remainder=False).map(map_fn)\n","\n","print(\"\\n3. Dataset + batch + map\")\n","for el in range_dataset:\n","    print(el)\n","\n","# Filter dataset based on a condition\n","def filter_fn(x):\n","    return tf.equal(tf.math.mod(x, 2), 0)\n","\n","range_dataset = tf.data.Dataset.range(0, 20, 1).filter(filter_fn)\n","\n","print(\"\\n4. Dataset + filter\")\n","for el in range_dataset:\n","    print(el)\n","\n","# Random shuffling\n","range_dataset = tf.data.Dataset.range(0, 20, 1).shuffle(\n","    buffer_size=20, reshuffle_each_iteration=False, seed=1234).batch(bs)\n","\n","print(\"\\n5. Dataset + shuffle + batch\")\n","for el in range_dataset:\n","    print(el)\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Dataset.range examples:\n","-----------------------\n","\n","1. Dataset\n","tf.Tensor(0, shape=(), dtype=int64)\n","tf.Tensor(1, shape=(), dtype=int64)\n","tf.Tensor(2, shape=(), dtype=int64)\n","tf.Tensor(3, shape=(), dtype=int64)\n","tf.Tensor(4, shape=(), dtype=int64)\n","tf.Tensor(5, shape=(), dtype=int64)\n","tf.Tensor(6, shape=(), dtype=int64)\n","tf.Tensor(7, shape=(), dtype=int64)\n","tf.Tensor(8, shape=(), dtype=int64)\n","tf.Tensor(9, shape=(), dtype=int64)\n","tf.Tensor(10, shape=(), dtype=int64)\n","tf.Tensor(11, shape=(), dtype=int64)\n","tf.Tensor(12, shape=(), dtype=int64)\n","tf.Tensor(13, shape=(), dtype=int64)\n","tf.Tensor(14, shape=(), dtype=int64)\n","tf.Tensor(15, shape=(), dtype=int64)\n","tf.Tensor(16, shape=(), dtype=int64)\n","tf.Tensor(17, shape=(), dtype=int64)\n","tf.Tensor(18, shape=(), dtype=int64)\n","tf.Tensor(19, shape=(), dtype=int64)\n","\n","2. Dataset + batch\n","tf.Tensor([0 1 2], shape=(3,), dtype=int64)\n","tf.Tensor([3 4 5], shape=(3,), dtype=int64)\n","tf.Tensor([6 7 8], shape=(3,), dtype=int64)\n","tf.Tensor([ 9 10 11], shape=(3,), dtype=int64)\n","tf.Tensor([12 13 14], shape=(3,), dtype=int64)\n","tf.Tensor([15 16 17], shape=(3,), dtype=int64)\n","tf.Tensor([18 19], shape=(2,), dtype=int64)\n","\n","3. Dataset + batch + map\n","tf.Tensor([0 1 4], shape=(3,), dtype=int64)\n","tf.Tensor([ 9 16 25], shape=(3,), dtype=int64)\n","tf.Tensor([36 49 64], shape=(3,), dtype=int64)\n","tf.Tensor([ 81 100 121], shape=(3,), dtype=int64)\n","tf.Tensor([144 169 196], shape=(3,), dtype=int64)\n","tf.Tensor([225 256 289], shape=(3,), dtype=int64)\n","tf.Tensor([324 361], shape=(2,), dtype=int64)\n","\n","4. Dataset + filter\n","tf.Tensor(0, shape=(), dtype=int64)\n","tf.Tensor(2, shape=(), dtype=int64)\n","tf.Tensor(4, shape=(), dtype=int64)\n","tf.Tensor(6, shape=(), dtype=int64)\n","tf.Tensor(8, shape=(), dtype=int64)\n","tf.Tensor(10, shape=(), dtype=int64)\n","tf.Tensor(12, shape=(), dtype=int64)\n","tf.Tensor(14, shape=(), dtype=int64)\n","tf.Tensor(16, shape=(), dtype=int64)\n","tf.Tensor(18, shape=(), dtype=int64)\n","\n","5. Dataset + shuffle + batch\n","tf.Tensor([15 10  1], shape=(3,), dtype=int64)\n","tf.Tensor([13  8 19], shape=(3,), dtype=int64)\n","tf.Tensor([18  7  9], shape=(3,), dtype=int64)\n","tf.Tensor([16 11  5], shape=(3,), dtype=int64)\n","tf.Tensor([14  0  2], shape=(3,), dtype=int64)\n","tf.Tensor([ 3 12  6], shape=(3,), dtype=int64)\n","tf.Tensor([ 4 17], shape=(2,), dtype=int64)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"G6Izh9nJ9kp6","colab_type":"text"},"source":["### tf.data.Dataset.from_tensors"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"rGv8Fope9kp8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"bff9e4c3-aa9c-4f04-c1a9-ec69d02fff8d","executionInfo":{"status":"ok","timestamp":1574423773856,"user_tz":-60,"elapsed":9101,"user":{"displayName":"Paolo Roncaglioni","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC3-ttkSuo6iETyKDuUhOcjyUtrv79Oa99mCxYr9h4=s64","userId":"17876907246980835885"}}},"source":["# Create Dataset as unique element\n","# --------------------------------\n","from_tensors_dataset = tf.data.Dataset.from_tensors([1, 2, 3, 4, 5, 6, 7, 8, 9])\n","\n","print(\"Dataset.from_tensors example:\")\n","print(\"-----------------------------\")\n","for el in from_tensors_dataset:\n","    print(el)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Dataset.from_tensors example:\n","-----------------------------\n","tf.Tensor([1 2 3 4 5 6 7 8 9], shape=(9,), dtype=int32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4iEvT_qK9kqA","colab_type":"text"},"source":["### tf.data.Dataset.from_tensor_slices"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"I59jx3zU9kqD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":605},"outputId":"c7a9949f-1e66-4fde-bf87-b56a82a65197","executionInfo":{"status":"ok","timestamp":1574423773858,"user_tz":-60,"elapsed":9095,"user":{"displayName":"Paolo Roncaglioni","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC3-ttkSuo6iETyKDuUhOcjyUtrv79Oa99mCxYr9h4=s64","userId":"17876907246980835885"}}},"source":["# Create a Dataset of slices\n","# --------------------------\n","\n","# All the elements must have the same size in first dimension (axis 0)\n","from_tensor_slices_dataset = tf.data.Dataset.from_tensor_slices(\n","    (np.random.uniform(size=[10, 2, 2]), np.random.randint(10, size=[10])))\n","\n","print(\"Dataset.from_tensor_slices example:\")\n","print(\"-----------------------------\")\n","for el in from_tensor_slices_dataset:\n","    print(el)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Dataset.from_tensor_slices example:\n","-----------------------------\n","(<tf.Tensor: id=149, shape=(2, 2), dtype=float64, numpy=\n","array([[0.50092879, 0.50595999],\n","       [0.35067746, 0.81432196]])>, <tf.Tensor: id=150, shape=(), dtype=int64, numpy=8>)\n","(<tf.Tensor: id=151, shape=(2, 2), dtype=float64, numpy=\n","array([[0.02679761, 0.84978878],\n","       [0.88837963, 0.40132742]])>, <tf.Tensor: id=152, shape=(), dtype=int64, numpy=9>)\n","(<tf.Tensor: id=153, shape=(2, 2), dtype=float64, numpy=\n","array([[0.07427505, 0.85993118],\n","       [0.35662869, 0.70952474]])>, <tf.Tensor: id=154, shape=(), dtype=int64, numpy=4>)\n","(<tf.Tensor: id=155, shape=(2, 2), dtype=float64, numpy=\n","array([[7.70924972e-03, 9.79163487e-01],\n","       [6.84658410e-01, 7.01594996e-04]])>, <tf.Tensor: id=156, shape=(), dtype=int64, numpy=7>)\n","(<tf.Tensor: id=157, shape=(2, 2), dtype=float64, numpy=\n","array([[0.70579758, 0.18484865],\n","       [0.27876293, 0.36835746]])>, <tf.Tensor: id=158, shape=(), dtype=int64, numpy=6>)\n","(<tf.Tensor: id=159, shape=(2, 2), dtype=float64, numpy=\n","array([[0.61122212, 0.46875187],\n","       [0.68466538, 0.31482378]])>, <tf.Tensor: id=160, shape=(), dtype=int64, numpy=0>)\n","(<tf.Tensor: id=161, shape=(2, 2), dtype=float64, numpy=\n","array([[0.85534611, 0.64181558],\n","       [0.60243164, 0.20565657]])>, <tf.Tensor: id=162, shape=(), dtype=int64, numpy=9>)\n","(<tf.Tensor: id=163, shape=(2, 2), dtype=float64, numpy=\n","array([[0.69539899, 0.59629052],\n","       [0.49658963, 0.22319008]])>, <tf.Tensor: id=164, shape=(), dtype=int64, numpy=2>)\n","(<tf.Tensor: id=165, shape=(2, 2), dtype=float64, numpy=\n","array([[0.51139414, 0.58456955],\n","       [0.03732376, 0.92426357]])>, <tf.Tensor: id=166, shape=(), dtype=int64, numpy=3>)\n","(<tf.Tensor: id=167, shape=(2, 2), dtype=float64, numpy=\n","array([[0.79736448, 0.85565181],\n","       [0.407904  , 0.55157776]])>, <tf.Tensor: id=168, shape=(), dtype=int64, numpy=0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Yr29OI4K9kqH","colab_type":"text"},"source":["### tf.data.Dataset.zip"]},{"cell_type":"code","metadata":{"id":"rYWmP9ja9kqI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":219},"outputId":"08442369-86cc-4065-bd06-167ddc0a0e1a","executionInfo":{"status":"ok","timestamp":1574423773859,"user_tz":-60,"elapsed":9089,"user":{"displayName":"Paolo Roncaglioni","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC3-ttkSuo6iETyKDuUhOcjyUtrv79Oa99mCxYr9h4=s64","userId":"17876907246980835885"}}},"source":["# Combine multiple datasets\n","# -------------------------\n","x = tf.data.Dataset.from_tensor_slices(np.random.uniform(size=10))\n","y = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6, 7, 8, 9])\n","\n","zipped = tf.data.Dataset.zip((x, y))\n","\n","print(\"Dataset.from_tensors example:\")\n","print(\"-----------------------------\")\n","for el in zipped:\n","    print(el)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Dataset.from_tensors example:\n","-----------------------------\n","(<tf.Tensor: id=179, shape=(), dtype=float64, numpy=0.5929095463819035>, <tf.Tensor: id=180, shape=(), dtype=int32, numpy=1>)\n","(<tf.Tensor: id=181, shape=(), dtype=float64, numpy=0.8657181714348197>, <tf.Tensor: id=182, shape=(), dtype=int32, numpy=2>)\n","(<tf.Tensor: id=183, shape=(), dtype=float64, numpy=0.1278121554327969>, <tf.Tensor: id=184, shape=(), dtype=int32, numpy=3>)\n","(<tf.Tensor: id=185, shape=(), dtype=float64, numpy=0.9335594097511452>, <tf.Tensor: id=186, shape=(), dtype=int32, numpy=4>)\n","(<tf.Tensor: id=187, shape=(), dtype=float64, numpy=0.7498948364394142>, <tf.Tensor: id=188, shape=(), dtype=int32, numpy=5>)\n","(<tf.Tensor: id=189, shape=(), dtype=float64, numpy=0.12128855745825973>, <tf.Tensor: id=190, shape=(), dtype=int32, numpy=6>)\n","(<tf.Tensor: id=191, shape=(), dtype=float64, numpy=0.009881425532407007>, <tf.Tensor: id=192, shape=(), dtype=int32, numpy=7>)\n","(<tf.Tensor: id=193, shape=(), dtype=float64, numpy=0.8869183179623849>, <tf.Tensor: id=194, shape=(), dtype=int32, numpy=8>)\n","(<tf.Tensor: id=195, shape=(), dtype=float64, numpy=0.9464986937966694>, <tf.Tensor: id=196, shape=(), dtype=int32, numpy=9>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Bt7zodMi9kqP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":439},"outputId":"4db1d805-313c-4be9-d91c-5865502e77fa","executionInfo":{"status":"ok","timestamp":1574423773861,"user_tz":-60,"elapsed":9086,"user":{"displayName":"Paolo Roncaglioni","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC3-ttkSuo6iETyKDuUhOcjyUtrv79Oa99mCxYr9h4=s64","userId":"17876907246980835885"}}},"source":["# Iterate over range dataset\n","# --------------------------\n","\n","# for a in b\n","for el in zipped:\n","    print(el)\n","    \n","print('\\n')\n","    \n","# for a in enumerate(b)\n","for el_idx, el in enumerate(zipped):\n","    print(el)\n","    \n","print('\\n')\n","    \n","# get iterator\n","iterator = iter(zipped)\n","print(next(iterator))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["(<tf.Tensor: id=202, shape=(), dtype=float64, numpy=0.5929095463819035>, <tf.Tensor: id=203, shape=(), dtype=int32, numpy=1>)\n","(<tf.Tensor: id=204, shape=(), dtype=float64, numpy=0.8657181714348197>, <tf.Tensor: id=205, shape=(), dtype=int32, numpy=2>)\n","(<tf.Tensor: id=206, shape=(), dtype=float64, numpy=0.1278121554327969>, <tf.Tensor: id=207, shape=(), dtype=int32, numpy=3>)\n","(<tf.Tensor: id=208, shape=(), dtype=float64, numpy=0.9335594097511452>, <tf.Tensor: id=209, shape=(), dtype=int32, numpy=4>)\n","(<tf.Tensor: id=210, shape=(), dtype=float64, numpy=0.7498948364394142>, <tf.Tensor: id=211, shape=(), dtype=int32, numpy=5>)\n","(<tf.Tensor: id=212, shape=(), dtype=float64, numpy=0.12128855745825973>, <tf.Tensor: id=213, shape=(), dtype=int32, numpy=6>)\n","(<tf.Tensor: id=214, shape=(), dtype=float64, numpy=0.009881425532407007>, <tf.Tensor: id=215, shape=(), dtype=int32, numpy=7>)\n","(<tf.Tensor: id=216, shape=(), dtype=float64, numpy=0.8869183179623849>, <tf.Tensor: id=217, shape=(), dtype=int32, numpy=8>)\n","(<tf.Tensor: id=218, shape=(), dtype=float64, numpy=0.9464986937966694>, <tf.Tensor: id=219, shape=(), dtype=int32, numpy=9>)\n","\n","\n","(<tf.Tensor: id=225, shape=(), dtype=float64, numpy=0.5929095463819035>, <tf.Tensor: id=226, shape=(), dtype=int32, numpy=1>)\n","(<tf.Tensor: id=227, shape=(), dtype=float64, numpy=0.8657181714348197>, <tf.Tensor: id=228, shape=(), dtype=int32, numpy=2>)\n","(<tf.Tensor: id=229, shape=(), dtype=float64, numpy=0.1278121554327969>, <tf.Tensor: id=230, shape=(), dtype=int32, numpy=3>)\n","(<tf.Tensor: id=231, shape=(), dtype=float64, numpy=0.9335594097511452>, <tf.Tensor: id=232, shape=(), dtype=int32, numpy=4>)\n","(<tf.Tensor: id=233, shape=(), dtype=float64, numpy=0.7498948364394142>, <tf.Tensor: id=234, shape=(), dtype=int32, numpy=5>)\n","(<tf.Tensor: id=235, shape=(), dtype=float64, numpy=0.12128855745825973>, <tf.Tensor: id=236, shape=(), dtype=int32, numpy=6>)\n","(<tf.Tensor: id=237, shape=(), dtype=float64, numpy=0.009881425532407007>, <tf.Tensor: id=238, shape=(), dtype=int32, numpy=7>)\n","(<tf.Tensor: id=239, shape=(), dtype=float64, numpy=0.8869183179623849>, <tf.Tensor: id=240, shape=(), dtype=int32, numpy=8>)\n","(<tf.Tensor: id=241, shape=(), dtype=float64, numpy=0.9464986937966694>, <tf.Tensor: id=242, shape=(), dtype=int32, numpy=9>)\n","\n","\n","(<tf.Tensor: id=248, shape=(), dtype=float64, numpy=0.5929095463819035>, <tf.Tensor: id=249, shape=(), dtype=int32, numpy=1>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tgaDo2Xn9kqT","colab_type":"text"},"source":["# Example: Fashion MNIST - Multi-class classification\n","## Dataset"]},{"cell_type":"code","metadata":{"id":"3roYbTXh9kqV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":164},"outputId":"21966156-a2a1-4668-8a0a-459210d8326e","executionInfo":{"status":"ok","timestamp":1574423775400,"user_tz":-60,"elapsed":10618,"user":{"displayName":"Paolo Roncaglioni","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC3-ttkSuo6iETyKDuUhOcjyUtrv79Oa99mCxYr9h4=s64","userId":"17876907246980835885"}}},"source":["# Load built-in dataset\n","# ---------------------\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","32768/29515 [=================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26427392/26421880 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","8192/5148 [===============================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4423680/4422102 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lg9zTeiQ9kqY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"d501d94e-851d-4e4f-bbc2-6934d21d6d16","executionInfo":{"status":"ok","timestamp":1574423775403,"user_tz":-60,"elapsed":10615,"user":{"displayName":"Paolo Roncaglioni","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC3-ttkSuo6iETyKDuUhOcjyUtrv79Oa99mCxYr9h4=s64","userId":"17876907246980835885"}}},"source":["x_train.shape\n","y_train.shape"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(60000, 28, 28)"]},"metadata":{"tags":[]},"execution_count":9},{"output_type":"execute_result","data":{"text/plain":["(60000,)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"WVMw6SiS9kqe","colab_type":"code","colab":{}},"source":["# Split in training and validation sets\n","# e.g., 50000 samples for training and 10000 samples for validation\n","\n","x_valid = x_train[50000:, ...] \n","y_valid = y_train[50000:, ...] \n","\n","x_train = x_train[:50000, ...]\n","y_train = y_train[:50000, ...]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"AsEPnqqH9kqi","colab_type":"code","colab":{}},"source":["# Create Training Dataset object\n","# ------------------------------\n","train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","\n","# Shuffle\n","train_dataset = train_dataset.shuffle(buffer_size=x_train.shape[0])\n","\n","# Normalize images\n","def normalize_img(x_, y_):\n","    return tf.cast(x_, tf.float32) / 255., y_\n","\n","train_dataset = train_dataset.map(normalize_img)\n","\n","# 1-hot encoding <- for categorical cross entropy\n","def to_categorical(x_, y_):\n","    return x_, tf.one_hot(y_, depth=10)\n","\n","train_dataset = train_dataset.map(to_categorical)\n","\n","# Divide in batches\n","bs = 32\n","train_dataset = train_dataset.batch(bs)\n","\n","# Repeat\n","# Without calling the repeat function the dataset \n","# will be empty after consuming all the images\n","train_dataset = train_dataset.repeat()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N7-v1io29kql","colab_type":"code","colab":{}},"source":["# Create Validation Dataset  \n","# -----------------------\n","valid_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n","\n","# Normalize images\n","valid_dataset = valid_dataset.map(normalize_img)\n","\n","# 1-hot encoding\n","valid_dataset = valid_dataset.map(to_categorical)\n","\n","# Divide in batches\n","valid_dataset = valid_dataset.batch(1)\n","\n","# Repeat\n","valid_dataset = valid_dataset.repeat()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nz9U4psf9kqp","colab_type":"code","colab":{}},"source":["# Create Test Dataset\n","# -------------------\n","test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n","\n","test_dataset = test_dataset.map(normalize_img)\n","\n","test_dataset = test_dataset.map(to_categorical)\n","\n","test_dataset = test_dataset.batch(1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"mRaitaLP9kqu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":163},"outputId":"f159284d-5160-4315-9fce-b32fdbe9f86d","executionInfo":{"status":"ok","timestamp":1574423776130,"user_tz":-60,"elapsed":11312,"user":{"displayName":"Paolo Roncaglioni","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC3-ttkSuo6iETyKDuUhOcjyUtrv79Oa99mCxYr9h4=s64","userId":"17876907246980835885"}}},"source":["# Check that is everything is ok..\n","\n","iterator = iter(train_dataset)\n","sample, target = next(iterator)\n","\n","# Just for visualization purpouses\n","sample = sample[0, ...]  # select first image in the batch\n","sample = sample * 255  # denormalize\n","\n","from PIL import Image\n","img = Image.fromarray(np.uint8(sample))\n","img = img.resize([128, 128])\n","img\n","\n","target[0]  # select corresponding target"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAEMUlEQVR4nO3bTWtkRRQG4Gfspptu\nEmiITFAmOBAQZiEMDIwbRUVw4cqVaxE3Iu78E4IrceNCt/4DBUFwMRsHBMGFKAgDAyMjCUQSEtKk\nwUV9pK59bxJEqVnUgW6q6tbHy6mXc849Vfeaq8goFVaYwMf4Rqot8Ta81zviInnqSgD+R2kAqgMY\nX6lXh0mfwws4hnfgC9yBT/DR2oiLpLoGGoBrV+r1HG7Cm+JmH6dHW7CfWsY4hK/wM/xxydTVNdAA\nVAdwKQnfhzdE0/IACwpvNxKf7aeWCVxPtXvw2fD81TXQAPQ4o7yr78Ir8KO49fvx5ya+h1twX7RP\nS9EQLXEALxGM1Jdp7q6fqq6BBqA6gIsM0acENjkWuTnFQ/gztbwMP6SOC9EGbWOPyPINfNC/SHUN\nNACXRsUjwtZm+7SVCitiIDTHJmH/pwRrtMiTzLELv6/NX10DDUB1AEMkvCFQJ/i+jdS8/sa1TIV5\nqm2nli0i7aadWkeqa6ABqA5giIRzJeOmZW2bEG5lRxkicdgRabsQaXc/1W7nWkeqa6ABGOLAZiqM\nYIYT+FXc2mcV8bc5nhebHoqdlnCWJnqmf6HqGmgAqgMYIuGtVMjR+JSQcvoFXlf6t1dF7n0r8rfr\nP3OSfV2qa6ABGOJANhtLiv1bKBIF8qMHIgd2/TNqGqVOi/6FqmugAagOYIiET4vBzikFr3L+cSny\n74yQLZgTmJZH5NdKS0X83pXqGmgALjJEB8SNHCnOSFaErZ/l2lTc+pxMWqXn09R7q3+h6hpoAKoD\nGCLhSSpMCbzKbnFJYVo20/Nsf+adobO0Sj5j6Up1DTQA1QEMkXAm2r5jCm+Yu0/WClMKAmZLOCZY\nwm3akc0TCWCIA934JduXUfyZK9zeucxSS+44IZyd5PhovzOiugYagOoAeki4SwjJDonebhOPCC4v\n+8dRnqDj6XLlhMjmI5GbNzQSPnEAejiwQdixY4qMZQ6EMgemeUiurcreS4pofUJfkqC6BhqA6gB6\nSLhDoNUR8RLdmcK+nFIQcEQg2ZTAtDFFRLRLSGCt8twdqa6BBqCHA/l+wEGuzRXHY+uR8fmrV75W\nkCUbsq3815HqGmgAqgPoIWG+NjDLtVXqmbuvlN6wcyIyFmsnRP83jzP2JAmqa6ABqA5gKCxfiY7w\nESF3jsCtfBB3zqilyLYZ/kqFU2Jsv0i9b6+tVl0DDcCQN1wKdwQ8JnBgg+I2wPqQ/KI2oUgWPSbQ\nqnv+l6W6BhqA6gCG7pbviefHR2Jpj5BtzEQcd4ZMCA4yO9Ed+A7uigHeYm216hpoAHo4kLNC14mf\nrt1VpHZyDnNC4ZrmBIu0T2DNh7n3W8LxbouIGoAe6SHhdUIg8yL8Bq/9+wXuEG7aHea5O1JdAw1A\nDwfuETzO1//FAj8RPonap+/7y+oaaACqA/gbXh7H4LJX5MYAAAAASUVORK5CYII=\n","text/plain":["<PIL.Image.Image image mode=L size=128x128 at 0x7FC8181A7B00>"]},"metadata":{"tags":[]},"execution_count":14},{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: id=364, shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"t9bUDtBS9kqx","colab_type":"text"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"3VadXO0e9kqy","colab_type":"code","colab":{}},"source":["# Fashion MNIST classification\n","# ----------------------------\n","\n","# x: 28x28\n","# y: 10 classes\n","\n","# Create Model\n","# ------------\n","# e.g. in: 28x28 -> h: 10 units -> out: 10 units (number of classes) \n","\n","# Define Input keras tensor\n","x = tf.keras.Input(shape=[28, 28])\n","\n","# Define intermediate hidden layers and chain\n","flatten = tf.keras.layers.Flatten()(x)\n","h = tf.keras.layers.Dense(units=10, activation=tf.keras.activations.sigmoid)(flatten)\n","\n","# Define output layer and chain\n","out = tf.keras.layers.Dense(units=10, activation=tf.keras.activations.softmax)(h)\n","\n","# Create Model instance defining inputs and outputs\n","model = tf.keras.Model(inputs=x, outputs=out)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"cor1IQRf9kq2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"d05f1477-12fd-413c-ff71-403fff4162fd","executionInfo":{"status":"ok","timestamp":1574423776132,"user_tz":-60,"elapsed":11303,"user":{"displayName":"Paolo Roncaglioni","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC3-ttkSuo6iETyKDuUhOcjyUtrv79Oa99mCxYr9h4=s64","userId":"17876907246980835885"}}},"source":["# Visualize created model as a table\n","model.summary()\n","\n","# Visualize initialized weights\n","model.weights"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 28, 28)]          0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 784)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 10)                7850      \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 10)                110       \n","=================================================================\n","Total params: 7,960\n","Trainable params: 7,960\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[<tf.Variable 'dense/kernel:0' shape=(784, 10) dtype=float32, numpy=\n"," array([[-0.03036179, -0.06281489,  0.0257396 , ...,  0.03800482,\n","         -0.01909882,  0.05336354],\n","        [-0.00394604,  0.00170591, -0.0514029 , ...,  0.05030797,\n","          0.0382886 , -0.03114992],\n","        [ 0.0818864 ,  0.05883654,  0.04829731, ..., -0.05160644,\n","         -0.05101755,  0.05875803],\n","        ...,\n","        [ 0.05402381, -0.00915979, -0.02856489, ...,  0.04777338,\n","          0.05148549, -0.08004379],\n","        [ 0.03537666,  0.06979673, -0.05969963, ...,  0.01393002,\n","          0.06334465, -0.04543552],\n","        [ 0.03403018, -0.05196487, -0.00468538, ..., -0.01439087,\n","          0.05410584,  0.00825624]], dtype=float32)>,\n"," <tf.Variable 'dense/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n"," <tf.Variable 'dense_1/kernel:0' shape=(10, 10) dtype=float32, numpy=\n"," array([[ 0.10681671, -0.21565178,  0.54518306,  0.33767676, -0.09082237,\n","         -0.52187127,  0.12106484, -0.5191472 ,  0.5226437 , -0.21219748],\n","        [ 0.25416505,  0.11191785, -0.09809113, -0.3264152 ,  0.25440925,\n","         -0.49294007, -0.0512577 , -0.33606952,  0.24689281, -0.20821053],\n","        [-0.29728   , -0.32662296,  0.45996022, -0.13579571, -0.03329766,\n","          0.24442732, -0.16079873, -0.1618894 ,  0.42001778, -0.51496035],\n","        [ 0.5448096 , -0.5391234 , -0.12660852,  0.07686871, -0.23395827,\n","         -0.17641094,  0.03499007,  0.15497702,  0.00352871, -0.50480026],\n","        [ 0.28958637, -0.49887916,  0.5176666 ,  0.09415799, -0.54766876,\n","         -0.4372638 ,  0.3983506 ,  0.3647986 , -0.01561809,  0.33814573],\n","        [-0.1780693 ,  0.54150176,  0.10118383,  0.4683069 ,  0.22114873,\n","          0.2281444 ,  0.21671736, -0.48920813,  0.36766064, -0.00699753],\n","        [ 0.47458422,  0.49016976, -0.08482856,  0.20801061, -0.33792675,\n","         -0.30647153,  0.10554636, -0.2665343 ,  0.03095192, -0.11156419],\n","        [-0.1206241 ,  0.16199297,  0.31660575, -0.08503553,  0.45165688,\n","         -0.40501696,  0.5115924 ,  0.29534042,  0.17607939,  0.3442729 ],\n","        [-0.21178561, -0.28189176,  0.47168422,  0.06032556, -0.1062856 ,\n","          0.28818172, -0.33715773, -0.09632024, -0.41079062, -0.31406647],\n","        [ 0.3211699 ,  0.07407689,  0.47473812,  0.4545287 ,  0.279549  ,\n","         -0.35434338, -0.4861652 , -0.4469988 ,  0.02550262,  0.0224725 ]],\n","       dtype=float32)>,\n"," <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"FzscvQ_Z9kq6","colab_type":"code","colab":{}},"source":["# Equivalent formulation\n","# ----------------------\n","\n","# Create model with sequential \n","# (uncomment to run)\n","# seq_model = tf.keras.Sequential()\n","# seq_model.add(tf.keras.layers.Flatten(input_shape=(28, 28))) # or as a list\n","# seq_model.add(tf.keras.layers.Dense(units=10, activation=tf.keras.activations.sigmoid))\n","# seq_model.add(tf.keras.layers.Dense(units=10, activation=tf.keras.activations.softmax))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gpne-LU59kq-","colab_type":"code","colab":{}},"source":["# seq_model.summary()\n","# seq_model.weights"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tv-DGBUJ9krB","colab_type":"text"},"source":["## Prepare the model for training"]},{"cell_type":"code","metadata":{"id":"S1WJWVLb9krC","colab_type":"code","colab":{}},"source":["# Optimization params\n","# -------------------\n","\n","# Loss\n","loss = tf.keras.losses.CategoricalCrossentropy()\n","\n","# learning rate\n","lr = 1e-4\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","# -------------------\n","\n","# Validation metrics\n","# ------------------\n","\n","metrics = ['accuracy']\n","# ------------------\n","\n","# Compile Model\n","model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pATv5pv79krG","colab_type":"text"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"nRVukBES9krH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":293},"outputId":"79cbdcc0-b2c5-44c5-e15d-c5a9705881e2"},"source":["model.fit(x=train_dataset,  # you can give directly numpy arrays x_train\n","          y=None,   # if x is a Dataset y has to be None, y_train otherwise\n","          epochs=10, \n","          steps_per_epoch=int(np.ceil(x_train.shape[0] / bs)),  # how many batches per epoch\n","          validation_data=valid_dataset,  # give a validation Dataset if you created it manually, otherwise you can use 'validation_split' for automatic split\n","          validation_steps=10000)  # number of batches in validation set"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train for 1563 steps, validate for 10000 steps\n","Epoch 1/10\n","1563/1563 [==============================] - 22s 14ms/step - loss: 1.9828 - accuracy: 0.3064 - val_loss: 1.7295 - val_accuracy: 0.5334\n","Epoch 2/10\n","1563/1563 [==============================] - 21s 13ms/step - loss: 1.5945 - accuracy: 0.6262 - val_loss: 1.4654 - val_accuracy: 0.6894\n","Epoch 3/10\n","1563/1563 [==============================] - 22s 14ms/step - loss: 1.3619 - accuracy: 0.7067 - val_loss: 1.2464 - val_accuracy: 0.7282\n","Epoch 4/10\n","1563/1563 [==============================] - 21s 13ms/step - loss: 1.1625 - accuracy: 0.7345 - val_loss: 1.0865 - val_accuracy: 0.7499\n","Epoch 5/10\n","1563/1563 [==============================] - 21s 13ms/step - loss: 1.0258 - accuracy: 0.7528 - val_loss: 0.9725 - val_accuracy: 0.7577\n","Epoch 6/10\n","1563/1563 [==============================] - 21s 14ms/step - loss: 0.9236 - accuracy: 0.7622 - val_loss: 0.8840 - val_accuracy: 0.7640\n","Epoch 7/10\n","1418/1563 [==========================>...] - ETA: 0s - loss: 0.8471 - accuracy: 0.7702"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aaB_xFSF9krK","colab_type":"text"},"source":["## Training with callbacks"]},{"cell_type":"code","metadata":{"id":"8XtFltqO9krM","colab_type":"code","colab":{}},"source":["import os\n","from datetime import datetime\n","\n","cwd = os.getcwd()\n","\n","exps_dir = os.path.join(cwd, 'experiments')\n","if not os.path.exists(exps_dir):\n","    os.makedirs(exps_dir)\n","\n","now = datetime.now().strftime('%b%d_%H-%M-%S')\n","\n","exp_dir = os.path.join(exps_dir, 'exp_' + str(now))\n","if not os.path.exists(exp_dir):\n","    os.makedirs(exp_dir)\n","\n","# Model checkpoint\n","# ----------------\n","ckpt_dir = os.path.join(exp_dir, 'ckpts')\n","if not os.path.exists(ckpt_dir):\n","    os.makedirs(ckpt_dir)\n","\n","ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'ckpt_{epoch:02d}'), \n","                                                   save_weights_only=True)  # False to save the model directly\n","# ----------------\n","\n","# Visualize Learning on Tensorboard\n","# ---------------------------------\n","tb_dir = os.path.join(exp_dir, 'tb_logs')\n","if not os.path.exists(tb_dir):\n","    os.makedirs(tb_dir)\n","    \n","# By default shows losses and metrics for both training and validation\n","tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n","                                             histogram_freq=1)  # if 1 shows weights histograms\n","# ---------------------------------\n","\n","model.fit(x=train_dataset,\n","          epochs=10,  #### set repeat in training dataset\n","          steps_per_epoch=int(np.ceil(x_train.shape[0] / bs)),\n","          validation_data=valid_dataset,\n","          validation_steps=10000, \n","          callbacks=[ckpt_callback, tb_callback])\n","\n","# How to visualize Tensorboard\n","\n","# 1. tensorboard --logdir EXPERIMENTS_DIR --port PORT     <- from terminal\n","# 2. localhost:PORT   <- in your browser"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1c7dMkJ89krR","colab_type":"text"},"source":["## Test model"]},{"cell_type":"code","metadata":{"id":"4Mh2lmfl9krS","colab_type":"code","colab":{}},"source":["# Let's try a different way to give data to model \n","# using directly the NumPy arrays\n","\n","# model.load_weights('/path/to/checkpoint')  # use this if you want to restore saved model\n","\n","eval_out = model.evaluate(x=x_test / 255.,\n","                          y=tf.keras.utils.to_categorical(y_test),\n","                          verbose=0)\n","\n","eval_out"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MtMOzYhT9krV","colab_type":"text"},"source":["## Compute prediction"]},{"cell_type":"code","metadata":{"id":"nVYd0oS09krV","colab_type":"code","colab":{}},"source":["# Compute output given x\n","\n","shoe_img = Image.open('shoe.png').convert('L')\n","\n","shoe_arr = np.expand_dims(np.array(shoe_img), 0)\n","\n","out_softmax = model.predict(x=shoe_arr / 255.)\n","\n","out_softmax  # is already a probability distribution (softmax)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z6a1A4OG9krZ","colab_type":"code","colab":{}},"source":["\n","out_softmax = tf.keras.activations.softmax(tf.convert_to_tensor(out_softmax))\n","out_softmax"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wo6KRw-W9krc","colab_type":"code","colab":{}},"source":["# Get predicted class as the index corresponding to the maximum value in the vector probability\n","predicted_class = tf.argmax(out_softmax, 1)\n","predicted_class"],"execution_count":0,"outputs":[]}]}